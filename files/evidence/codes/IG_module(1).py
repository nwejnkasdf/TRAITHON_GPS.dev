# IG_module.py
# A.X-4.0 Light ê¸°ë°˜ ë‚šì‹œì„± ê¸°ì‚¬ í† í° ê¸°ì—¬ë„(Integrated Gradients) ì „ìš© ëª¨ë“ˆ.

from typing import Dict, Any, Tuple, List

import torch
import numpy as np

# ì ìˆ˜ ì‚°ì •ìš© ê³µí†µ ë¡œì§ì€ ê¸°ì¡´ ëª¨ë“ˆì—ì„œ ê°€ì ¸ì˜¨ë‹¤.
try:
    from ax4_clickbait_scorer import init_model, _build_prefix_ids_for_binary, build_article_text
except ImportError:
    from ax4_clickbait_score import init_model, _build_prefix_ids_for_binary, build_article_text  # type: ignore

# ëª¨ë¸ / í† í¬ë‚˜ì´ì € / ë””ë°”ì´ìŠ¤ëŠ” score ëª¨ë“ˆê³¼ ì™„ì „íˆ ê³µìœ í•œë‹¤.
_MODEL, _TOKENIZER, _DEVICE, _DTYPE = init_model()

ARTICLE_START = "[ë³¸ë¬¸]"
ARTICLE_END = "ì´ ê¸°ì‚¬ê°€ ë‚šì‹œì„±ì´ ê°•í•˜ë©´"


def _make_attention_mask(prefix_ids: torch.Tensor) -> torch.Tensor:
    """
    prefix_ids (1, L)ì— ëŒ€í•´ attention maskë¥¼ ë§Œë“ ë‹¤.
    """
    if _TOKENIZER.pad_token_id is not None:
        attention_mask = (prefix_ids != _TOKENIZER.pad_token_id).long()
    else:
        attention_mask = torch.ones_like(prefix_ids)
    return attention_mask


def analyze_article_with_ig(article: Dict[str, Any], m_steps: int = 50) -> Dict[str, Any]:
    """
    í•œ ê¸°ì‚¬ì— ëŒ€í•´ Integrated Gradients (IG)ë¥¼ ì‚¬ìš©í•´
      - p0, p1 (ë‹¤ìŒ í† í°ì´ 0/1ì¼ í™•ë¥ )
      - score_logit_diff = logit(1) - logit(0)
      - í† í°ë³„ IG ê¸°ì—¬ë„ (token_scores)
    ë¥¼ í•œ ë²ˆì— ê³„ì‚°í•´ì„œ ë°˜í™˜í•œë‹¤.

    Args:
        article: newsCategory/newsTitle/newsContent í•„ë“œë¥¼ ê°€ì§„ ê¸°ì‚¬ JSON dict
        m_steps: IG ì ë¶„ ìŠ¤í… ìˆ˜ (í´ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë ¤ì§)
    """
    # 1) prefix í† í° ì‹œí€€ìŠ¤
    prefix_ids = _build_prefix_ids_for_binary(article)  # [1, L] (ì´ë¯¸ _DEVICEì— ì˜¬ë¼ê°€ ìˆë‹¤ê³  ê°€ì •)
    attention_mask = _make_attention_mask(prefix_ids).to(_DEVICE)

    # 2) 0/1 í† í° id
    id0 = _TOKENIZER("0", add_special_tokens=False).input_ids[0]
    id1 = _TOKENIZER("1", add_special_tokens=False).input_ids[0]

    # 3) ì„ë² ë”© ë ˆì´ì–´ì—ì„œ ì›ë³¸ ì…ë ¥ ì„ë² ë”©(real_embeds) ì–»ê¸°
    emb_layer = _MODEL.get_input_embeddings()
    with torch.no_grad():
        real_embeds = emb_layer(prefix_ids)  # [1, L, d]
    real_embeds = real_embeds.to(_DEVICE)

    # 4) ì›ë³¸ ì…ë ¥(alpha=1.0)ì— ëŒ€í•œ p0, p1, score ê³„ì‚°
    with torch.no_grad():
        outputs = _MODEL(
            inputs_embeds=real_embeds,
            attention_mask=attention_mask,
        )
        last_logits = outputs.logits[0, -1, :]  # [V]

    two = torch.stack([last_logits[id0], last_logits[id1]], dim=0)  # [2]
    probs = torch.softmax(two, dim=0)
    p0 = probs[0].item()
    p1 = probs[1].item()
    score_logit_diff = (last_logits[id1] - last_logits[id0]).item()

    # 5) Integrated Gradients (IG) ê³„ì‚°
    baseline_embeds = torch.zeros_like(real_embeds)
    accumulated_grads = torch.zeros_like(real_embeds)

    alphas = torch.linspace(0.0, 1.0, steps=m_steps).to(_DEVICE)

    for alpha in alphas:
        interpolated_embeds = baseline_embeds + alpha * (real_embeds - baseline_embeds)
        interpolated_embeds.requires_grad_(True)

        _MODEL.zero_grad()

        ig_outputs = _MODEL(
            inputs_embeds=interpolated_embeds,
            attention_mask=attention_mask,
        )
        ig_last_logits = ig_outputs.logits[0, -1, :]

        ig_score = ig_last_logits[id1] - ig_last_logits[id0]

        ig_score.backward()

        accumulated_grads += interpolated_embeds.grad.detach()

    # 6) í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸, IG = (real - baseline) * í‰ê·  grad
    avg_grads = accumulated_grads / float(m_steps)
    ig_attrib = (real_embeds - baseline_embeds) * avg_grads  # [1, L, d]

    # ğŸ”§ ì—¬ê¸°ì„œ bfloat16 â†’ float32 ë³€í™˜ (numpyê°€ ì§€ì›í•˜ëŠ” dtype)
    ig_attrib = ig_attrib.to(torch.float32)

    # 7) í† í°ë³„ ìŠ¤ì¹¼ë¼ ì ìˆ˜: ì„ë² ë”© ì°¨ì›ì— ëŒ€í•´ í•©ì‚°
    token_scores = ig_attrib.sum(dim=-1).squeeze(0)  # [L]
    token_scores = token_scores.detach().cpu().numpy()

    token_ids = prefix_ids[0].tolist()
    tokens = _TOKENIZER.convert_ids_to_tokens(token_ids)

    return {
        "p0": p0,
        "p1": p1,
        "score_logit_diff": score_logit_diff,
        "p_clickbait": p1,
        "tokens": tokens,
        "token_ids": token_ids,
        "token_scores": token_scores,
    }


def aggregate_to_words(attrib_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    í† í° ë‹¨ìœ„ IG ì ìˆ˜ë¥¼ "ë‹¨ì–´" ë‹¨ìœ„ë¡œ í•©ì³ì£¼ëŠ” í—¬í¼.
    - build_article_textì—ì„œ ì‚¬ìš©í•œ [ë³¸ë¬¸] ~ "ì´ ê¸°ì‚¬ê°€ ë‚šì‹œì„±ì´ ê°•í•˜ë©´ ..." êµ¬ê°„ë§Œ ì‚¬ìš©
    """
    tokens: List[str] = attrib_result["tokens"]
    token_ids: List[int] = attrib_result["token_ids"]
    scores: np.ndarray = np.asarray(attrib_result["token_scores"])

    words: List[Tuple[str, float]] = []
    cur_text = ""
    cur_score = 0.0

    running_text = ""
    in_article = False
    finished = False

    for tok, tid, s in zip(tokens, token_ids, scores):
        piece = _TOKENIZER.decode([tid])
        running_text += piece

        # [ë³¸ë¬¸] ì´ì „ í† í°ì€ ë¬´ì‹œ
        if not in_article:
            if ARTICLE_START in running_text:
                in_article = True
            else:
                continue

        # ì§€ì‹œë¬¸ì´ ì‹œì‘ë˜ë©´ ë³¸ë¬¸ ì¢…ë£Œ
        if ARTICLE_END in running_text:
            finished = True
        if finished:
            break

        # ëŒ€ê´„í˜¸ ë§ˆì»¤ëŠ” ë‹¨ì–´ì—ì„œ ì œì™¸
        if "[" in piece or "]" in piece:
            if cur_text:
                words.append((cur_text, cur_score))
                cur_text, cur_score = "", 0.0
            continue

        piece_strip = piece.strip()
        if not piece_strip:
            continue

        new_word = tok.startswith("â–") or piece.startswith(" ")

        if new_word:
            if cur_text:
                words.append((cur_text, cur_score))
            cur_text = piece_strip
            cur_score = float(s)
        else:
            cur_text += piece_strip
            cur_score += float(s)

    if cur_text:
        words.append((cur_text, cur_score))

    word_tokens = [w for (w, _) in words]
    word_scores = [float(v) for (_, v) in words]

    result = dict(attrib_result)
    result["word_tokens"] = word_tokens
    result["word_scores"] = word_scores
    return result


def print_top_words(attrib_result: Dict[str, Any], top_k: int = 20) -> None:
    """
    aggregate_to_words ê²°ê³¼ë¥¼ ì´ìš©í•´ IG ì ˆëŒ“ê°’ ê¸°ì¤€ ìƒìœ„ ë‹¨ì–´ë¥¼ ì¶œë ¥í•˜ëŠ” ìœ í‹¸.
    """
    agg = aggregate_to_words(attrib_result)
    pairs = list(zip(agg["word_tokens"], agg["word_scores"]))

    pairs_sorted = sorted(pairs, key=lambda x: abs(x[1]), reverse=True)

    print(f"=== Top {top_k} words by |IG score| ===")
    for word, score in pairs_sorted[:top_k]:
        print(f"{word}\t{score:.4f}")


if __name__ == "__main__":
    import sys
    import json

    if len(sys.argv) != 2:
        print("Usage: python IG_module.py /path/to/article.json")
        sys.exit(0)

    path = sys.argv[1]

    with open(path, "r", encoding="utf-8") as f:
        article = json.load(f)

    attrib = analyze_article_with_ig(article, m_steps=50)
    print_top_words(attrib, top_k=30)
